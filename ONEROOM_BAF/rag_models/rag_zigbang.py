import os
import json
import re
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores.faiss import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyMuPDFLoader

# í™˜ê²½ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "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"
embedding = OpenAIEmbeddings()

def safe_get(d, key):
    val = d.get(key, "")
    return str(val).strip().replace("\n", " ") if val is not None else ""

def safe_int(val):
    try:
        return int(val)
    except:
        return 999

def convert_json_docs_to_text(json_docs):
    documents = []
    for doc in json_docs:
        try:
            content = json.loads(doc.page_content) if isinstance(doc.page_content, str) else doc.page_content

            description = safe_get(content, 'ì„¤ëª…')
            title = safe_get(content, 'ì œëª©')
            location = safe_get(content, 'ì£¼ì†Œ(ë²•ì •ë™)')
            deposit = safe_get(content, 'ë³´ì¦ê¸ˆ(ë§Œì›)')
            rent = safe_get(content, 'ì›”ì„¸(ë§Œì›)')
            area = safe_get(content, 'ì „ìš©ë©´ì (mÂ²)')
            room_type = safe_get(content, 'ë°©ì¢…ë¥˜')
            room_layout = safe_get(content, 'ë£¸íƒ€ìž…')
            parking = safe_get(content, 'ì£¼ì°¨ì—¬ë¶€')
            floor = safe_get(content, 'ì¸µìˆ˜')
            options = safe_get(content, 'ì˜µì…˜')
            available_date = safe_get(content, 'ìž…ì£¼ê°€ëŠ¥ì¼')
            nearest_station = safe_get(content, 'ê°€ìž¥ê°€ê¹Œìš´ì—­')

            time_to_chungmuro = safe_int(content.get('ë§¤ë¬¼_ë¶€í„°_ì¶©ë¬´ë¡œ1ì¶œê¹Œì§€_ì‹œê°„_ë¶„'))
            time_to_dongguk = safe_int(content.get('ë§¤ë¬¼_ë¶€í„°_ë™ìž…6ì¶œê¹Œì§€_ì‹œê°„_ë¶„'))

            text = f"""
[{title}]
- ì„¤ëª…: {description}
- ìœ„ì¹˜: {location}
- ë³´ì¦ê¸ˆ/ì›”ì„¸: {deposit}/{rent}ë§Œì›
- ë©´ì : {area}ãŽ¡
- ë°©ì¢…ë¥˜: {room_type}, ë£¸íƒ€ìž…: {room_layout}
- ì£¼ì°¨: {parking}, ì¸µìˆ˜: {floor}
- ì˜µì…˜: {options}
- ìž…ì£¼ ê°€ëŠ¥ì¼: {available_date}
- ê°€ìž¥ ê°€ê¹Œìš´ ì—­: {nearest_station}
- ì¶©ë¬´ë¡œì—­ê¹Œì§€ ì‹œê°„: {time_to_chungmuro}ë¶„
- ë™ëŒ€ìž…êµ¬ì—­ê¹Œì§€ ì‹œê°„: {time_to_dongguk}ë¶„
""".strip()

            documents.append(Document(
                page_content=text,
                metadata={
                    "ë§¤ë¬¼ID": content.get("ë§¤ë¬¼ID", ""),
                    "ì¶©ë¬´ë¡œ_ì†Œìš”ì‹œê°„_ë¶„": time_to_chungmuro,
                    "ë™ëŒ€ìž…êµ¬_ì†Œìš”ì‹œê°„_ë¶„": time_to_dongguk
                }
            ))
        except Exception as e:
            print("âŒ ë³€í™˜ ì‹¤íŒ¨:", e)

    return documents

    
# ì§ˆì˜ ë¶„ì„ í•¨ìˆ˜ë“¤
def extract_station_and_minutes(query: str):
    station_match = re.search(r'([ê°€-íž£]+)ì—­', query)
    time_match = re.search(r'(\d+)\s*ë¶„\s*ì´ë‚´', query)
    return {
        "ì—­": station_match.group(1) if station_match else None,
        "ë¶„": int(time_match.group(1)) if time_match else None
    }

def extract_deposit_limit(query: str):
    match = re.search(r'ë³´ì¦ê¸ˆ\s*(\d{2,5})\s*ë§Œì›\s*(ì´ë‚´|ê¹Œì§€)?', query)
    return int(match.group(1)) if match else None

def classify_query(query: str):
    query = query.lower()
    legal_keywords = ["ëŒë ¤ë°›", "ëª» ë°›", "ì†Œì†¡", "ê³„ì•½", "íŒŒê¸°", "ë²•ë¥ ", "ë¬¸ì œ", "ë¶„ìŸ", "ì£¼ì˜ì‚¬í•­"]
    housing_keywords = ["ë³´ì¦ê¸ˆ", "ì›”ì„¸", "ì—­", "ë§¤ë¬¼", "ë©´ì ", "ì£¼ì°¨", "ì›ë£¸", "íˆ¬ë£¸","ì£¼ì°¨","ê°€ê¹Œìš´"]
    if any(k in query for k in legal_keywords):
        return "pdf"
    elif any(k in query for k in housing_keywords):
        return "csv"
    return "pdf"

# ë§¤ë¬¼ JSON ë¡œë”© ë° í•„í„°ë§ ì²˜ë¦¬
def load_json_to_documents(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    id_to_raw = {entry.get("ë§¤ë¬¼ID"): entry for entry in raw_data}
    
    # JSONì„ Documentë¡œ ë³€í™˜
    raw_docs = [
        Document(page_content=json.dumps(entry), metadata={"ë§¤ë¬¼ID": entry.get("ë§¤ë¬¼ID")})
        for entry in raw_data
    ]
    
    # ìžì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    converted_docs = convert_json_docs_to_text(raw_docs)
    return converted_docs, id_to_raw



def normalize_station_name(name):
    return name.replace("ì—­", "").strip() if name else ""

def filter_docs(docs, id_to_raw, query):
    parsed = extract_station_and_minutes(query)
    deposit_limit = extract_deposit_limit(query)
    station_name = normalize_station_name(parsed['ì—­'])
    max_minutes = parsed['ë¶„'] if parsed['ë¶„'] else 999

    filtered = []
    for doc in docs:
        doc_id = doc.metadata.get("ë§¤ë¬¼ID")
        raw = id_to_raw.get(doc_id, {})

        try:
            deposit = int(str(raw.get("ë³´ì¦ê¸ˆ(ë§Œì›)", "99999")).replace(",", ""))
            if deposit_limit and deposit > deposit_limit:
                continue
        except:
            continue

        time_value = doc.metadata.get(f"{station_name}_ì†Œìš”ì‹œê°„_ë¶„", 999)
        closest_station = normalize_station_name(raw.get("ê°€ìž¥ê°€ê¹Œìš´ì—­", ""))

        if station_name:
            is_close_by_name = station_name in closest_station
            is_within_time = time_value <= max_minutes
            if not (is_close_by_name or is_within_time):
                continue

        filtered.append(doc)

    return filtered



# CSV ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë”©
def get_csv_qa(json_path, vector_path, query):
    docs, id_to_raw = load_json_to_documents(json_path)
    filtered_docs = filter_docs(docs, id_to_raw, query)

    if not filtered_docs:
        print("â—ì¡°ê±´ì— ë§žëŠ” ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None, id_to_raw

    print(f"ðŸŽ¯ ì¡°ê±´ì— ë§žëŠ” ë§¤ë¬¼ ìˆ˜: {len(filtered_docs)}")

    
    splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
    split_docs = splitter.split_documents(filtered_docs)
    vs = FAISS.from_documents(split_docs, embedding)
    vs.save_local(vector_path)

    prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë§¤ë¬¼ ì¶”ì²œì„ ë„ì™€ì£¼ëŠ” ì±—ë´‡ìž…ë‹ˆë‹¤.
    ë‹¤ìŒì€ ë§¤ë¬¼ ë°ì´í„°ìž…ë‹ˆë‹¤.
    ë¬¸ì„œì— ë³´ì¦ê¸ˆ, ê±°ë¦¬ ë“±ì˜ ì¡°ê±´ì´ ì–¸ê¸‰ë˜ì–´ ìžˆìœ¼ë©´ ê·¸ì— ë§žëŠ” ë§¤ë¬¼ì„ ê³¨ë¼ì„œ ë¶€ë“œëŸ½ê³  ìžì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ê°„ë‹¨ížˆ ìš”ì•½í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    

    
    


    [ë¬¸ì„œ ë‚´ìš©]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """
    )    

    qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    retriever=vs.as_retriever(search_kwargs={"k": 10}),
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},  # ì—¬ê¸°ì— í”„ë¡¬í”„íŠ¸ ë„£ê¸°
    return_source_documents=True
    )
    
    return qa, id_to_raw



# PDF ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„±
def get_pdf_qa(pdf_path):
    loader = PyMuPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = []
    for doc in docs:
        page = doc.metadata.get("page")
        for chunk in splitter.split_text(doc.page_content):
            chunks.append(Document(
                page_content=chunk,
                metadata={"page": page, "source": chunk[:30].strip().replace("\n", " ")}
            ))
    vectordb = FAISS.from_documents(chunks, embedding)
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
ë‹¤ìŒì€ 'ìžì·¨ë°±ê³¼ì‚¬ì „'ì˜ ë‚´ìš©ìž…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì•„ëž˜ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""
    )
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0),
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    return qa

# í†µí•© ì±—ë´‡

def unified_chatbot(query: str):
    print(f"\nðŸ’¬ ì‚¬ìš©ìž ì§ˆë¬¸: {query}")
    source = classify_query(query)
    if source == "csv":
        qa, id_to_raw = get_csv_qa("./data/ë§¤ë¬¼_ë°ì´í„°.json", "./vectorstore", query)
        result = qa.invoke(query)

        # âœ… ì¡°ê±´ì— ë§žëŠ” ë§¤ë¬¼ IDë§Œ í•„í„°ë§
        allowed_ids = set(doc.metadata.get("ë§¤ë¬¼ID") for doc in result['source_documents'])

        print("\nðŸ“¦ ë§¤ë¬¼ ê²°ê³¼:")
        count = 0
        for i, doc in enumerate(result['source_documents']):
            doc_id = doc.metadata.get("ë§¤ë¬¼ID")
            if doc_id not in allowed_ids:
                continue
            raw = id_to_raw.get(doc_id, {})
            print(f"â–¶ï¸ ë§¤ë¬¼ {count+1}")
            print(f"- ìœ„ì¹˜: {raw.get('ì£¼ì†Œ(ë²•ì •ë™)', 'ì •ë³´ ì—†ìŒ')} / ë³´ì¦ê¸ˆ: {raw.get('ë³´ì¦ê¸ˆ(ë§Œì›)', 'ì •ë³´ ì—†ìŒ')} / ì›”ì„¸: {raw.get('ì›”ì„¸(ë§Œì›)', 'ì •ë³´ ì—†ìŒ')}ë§Œì›")
            print(f"- ì¶©ë¬´ë¡œì—­ê¹Œì§€ ì‹œê°„: {raw.get('ë§¤ë¬¼_ë¶€í„°_ì¶©ë¬´ë¡œ1ì¶œê¹Œì§€_ì‹œê°„_ë¶„', 'ì •ë³´ ì—†ìŒ')}ë¶„")
            print("-" * 40)
            count += 1

        print("\nðŸ§  LLM ì‘ë‹µ:", result['result'])
    else:
        qa = get_pdf_qa(r"C:\\Users\\jeong\\baf_langchain\\data\\á„Œá…¡á„Žá…±á„‡á…¢á†¨á„€á…ªá„‰á…¡á„Œá…¥á†«_2025 (1).pdf")
        result = qa.invoke(query)
        print("\nðŸ“˜ ìžì·¨ë°±ê³¼ ì‘ë‹µ:", result['result'])
        print("\nðŸ“„ ì¶œì²˜ ë¬¸ì„œ:")
        for i, doc in enumerate(result['source_documents']):
            print(f"--- ì¶œì²˜ {i+1} ---\n{doc.page_content[:300]}\n")

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    test_queries = [
        "ë¶€ë™ì‚° ì¤‘ê°œë³´ìˆ˜ì˜ ìµœê³  ìš”ìœ¨ì´ëž€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?"
        
       
    ]
    for q in test_queries:
        unified_chatbot(q)
        print("\n" + "=" * 80 + "\n")
