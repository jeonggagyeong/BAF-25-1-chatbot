import os
import json
import re
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores.faiss import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyMuPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
embedding = OpenAIEmbeddings(openai_api_key=api_key)


def safe_get(d, key):
    val = d.get(key, "")
    return str(val).strip().replace("\n", " ") if val is not None else ""


def safe_int(val):
    try:
        return int(val)
    except:
        return 999  # fallback for filtering


from langchain.schema import Document


def convert_json_docs_to_text(json_docs):
    documents = []
    for doc in json_docs:
        try:
            content = json.loads(doc.page_content) if isinstance(doc.page_content, str) else doc.page_content

            description = safe_get(content, 'ì„¤ëª…')
            title = safe_get(content, 'ì œëª©')
            location = safe_get(content, 'ì£¼ì†Œ(ë²•ì •ë™)')
            deposit = safe_get(content, 'ë³´ì¦ê¸ˆ(ë§Œì›)')
            rent = safe_get(content, 'ì›”ì„¸(ë§Œì›)')
            area = safe_get(content, 'ì „ìš©ë©´ì (mÂ²)')
            room_type = safe_get(content, 'ë°©ì¢…ë¥˜')
            room_layout = safe_get(content, 'ë£¸íƒ€ì…')
            parking = safe_get(content, 'ì£¼ì°¨ì—¬ë¶€')
            floor = safe_get(content, 'ì¸µìˆ˜')
            options = safe_get(content, 'ì˜µì…˜')
            available_date = safe_get(content, 'ì…ì£¼ê°€ëŠ¥ì¼')
            nearest_station = safe_get(content, 'ê°€ì¥ê°€ê¹Œìš´ì—­')

            time_to_chungmuro = safe_int(content.get('ë§¤ë¬¼_ë¶€í„°_ì¶©ë¬´ë¡œ1ì¶œê¹Œì§€_ì‹œê°„_ë¶„'))
            time_to_dongguk = safe_int(content.get('ë§¤ë¬¼_ë¶€í„°_ë™ì…6ì¶œê¹Œì§€_ì‹œê°„_ë¶„'))

            text = f"""
[{title}]
- ì„¤ëª…: {description}
- ìœ„ì¹˜: {location}
- ë³´ì¦ê¸ˆ/ì›”ì„¸: {deposit}/{rent}ë§Œì›
- ë©´ì : {area}ã¡
- ë°©ì¢…ë¥˜: {room_type}, ë£¸íƒ€ì…: {room_layout}
- ì£¼ì°¨: {parking}, ì¸µìˆ˜: {floor}
- ì˜µì…˜: {options}
- ì…ì£¼ ê°€ëŠ¥ì¼: {available_date}
- ê°€ì¥ ê°€ê¹Œìš´ ì—­: {nearest_station}
- ì¶©ë¬´ë¡œì—­ê¹Œì§€ ì‹œê°„: {time_to_chungmuro}ë¶„
- ë™ëŒ€ì…êµ¬ì—­ê¹Œì§€ ì‹œê°„: {time_to_dongguk}ë¶„
""".strip()

            documents.append(Document(
                page_content=text,
                metadata={
                    "ë§¤ë¬¼ID": content.get("ë§¤ë¬¼ID", ""),
                    "ì¶©ë¬´ë¡œ_ì†Œìš”ì‹œê°„_ë¶„": time_to_chungmuro,
                    "ë™ëŒ€ì…êµ¬_ì†Œìš”ì‹œê°„_ë¶„": time_to_dongguk
                }
            ))
        except Exception as e:
            print("âŒ ë³€í™˜ ì‹¤íŒ¨:", e)

    return documents


# âœ… ì§ˆì˜ ë¶„ì„ í•¨ìˆ˜ë“¤
def extract_station_and_minutes(query: str):
    station_match = re.search(r'([ê°€-í£]+)ì—­', query)
    time_match = re.search(r'(\d+)\s*ë¶„\s*ì´ë‚´', query)
    return {
        "ì—­": station_match.group(1) if station_match else None,
        "ë¶„": int(time_match.group(1)) if time_match else None
    }


def extract_deposit_limit(query: str):
    match = re.search(r'ë³´ì¦ê¸ˆ\s*(\d{2,5})\s*ë§Œì›\s*(ì´ë‚´|ê¹Œì§€)?', query)
    return int(match.group(1)) if match else None


def classify_query(query: str):
    query = query.lower()
    legal_keywords = ["ëŒë ¤ë°›", "ëª» ë°›", "ì†Œì†¡", "ê³„ì•½", "íŒŒê¸°", "ë²•ë¥ ", "ë¬¸ì œ", "ë¶„ìŸ", "ì£¼ì˜ì‚¬í•­"]
    housing_keywords = ["ë³´ì¦ê¸ˆ", "ì›”ì„¸", "ì—­", "ë§¤ë¬¼", "ë©´ì ", "ì£¼ì°¨", "ì›ë£¸", "íˆ¬ë£¸", "ì£¼ì°¨", "ê°€ê¹Œìš´"]
    if any(k in query for k in legal_keywords):
        return "pdf"
    elif any(k in query for k in housing_keywords):
        return "csv"
    return "pdf"


# âœ… ë§¤ë¬¼ JSON ë¡œë”© ë° í•„í„°ë§ ì²˜ë¦¬
def load_json_to_documents(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    id_to_raw = {entry.get("ë§¤ë¬¼ID"): entry for entry in raw_data}

    # JSONì„ Documentë¡œ ë³€í™˜
    raw_docs = [
        Document(page_content=json.dumps(entry), metadata={"ë§¤ë¬¼ID": entry.get("ë§¤ë¬¼ID")})
        for entry in raw_data
    ]

    # ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    converted_docs = convert_json_docs_to_text(raw_docs)
    return converted_docs, id_to_raw


def normalize_station_name(name):
    return name.replace("ì—­", "").strip() if name else ""


def filter_docs(docs, id_to_raw, query):
    parsed = extract_station_and_minutes(query)
    deposit_limit = extract_deposit_limit(query)
    station_name = normalize_station_name(parsed['ì—­'])
    max_minutes = parsed['ë¶„'] if parsed['ë¶„'] else 999

    filtered = []
    for doc in docs:
        doc_id = doc.metadata.get("ë§¤ë¬¼ID")
        raw = id_to_raw.get(doc_id, {})

        try:
            deposit = int(str(raw.get("ë³´ì¦ê¸ˆ(ë§Œì›)", "99999")).replace(",", ""))
            if deposit_limit and deposit > deposit_limit:
                continue
        except:
            continue

        time_value = doc.metadata.get(f"{station_name}_ì†Œìš”ì‹œê°„_ë¶„", 999)
        closest_station = normalize_station_name(raw.get("ê°€ì¥ê°€ê¹Œìš´ì—­", ""))

        if station_name:
            is_close_by_name = station_name in closest_station
            is_within_time = time_value <= max_minutes
            if not (is_close_by_name or is_within_time):
                continue

        filtered.append(doc)

    return filtered


# âœ… CSV ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë”©
def get_csv_qa(json_path, vector_path, query):
    docs, id_to_raw = load_json_to_documents(json_path)
    filtered_docs = filter_docs(docs, id_to_raw, query)

    if not filtered_docs:
        print("â—ì¡°ê±´ì— ë§ëŠ” ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None, id_to_raw

    print(f"ğŸ¯ ì¡°ê±´ì— ë§ëŠ” ë§¤ë¬¼ ìˆ˜: {len(filtered_docs)}")

    if os.path.exists(os.path.join(vector_path, "index.faiss")):
        vs = FAISS.load_local(vector_path, embedding, allow_dangerous_deserialization=True)
    else:
        splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
        split_docs = splitter.split_documents(filtered_docs)
        vs = FAISS.from_documents(split_docs, embedding)
        vs.save_local(vector_path)

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
    ë‹¤ìŒì€ ë§¤ë¬¼ ë°ì´í„°ì…ë‹ˆë‹¤.
    ë¬¸ì„œì— ë³´ì¦ê¸ˆ, ê±°ë¦¬ ë“±ì˜ ì¡°ê±´ì´ ì–¸ê¸‰ë˜ì–´ ìˆìœ¼ë©´ ê·¸ì— ë§ëŠ” ë§¤ë¬¼ì„ ê³¨ë¼ì„œ ìš”ì•½í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.

    [ë¬¸ì„œ ë‚´ìš©]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """
    )

    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0),
        retriever=vs.as_retriever(search_kwargs={"k": 10}),
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},  # âœ… ì—¬ê¸°ì— í”„ë¡¬í”„íŠ¸ ë„£ê¸°
        return_source_documents=True
    )

    return qa, id_to_raw


# âœ… PDF ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„±
def get_pdf_qa(pdf_path):
    loader = PyMuPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = []
    for doc in docs:
        page = doc.metadata.get("page")
        for chunk in splitter.split_text(doc.page_content):
            chunks.append(Document(
                page_content=chunk,
                metadata={"page": page, "source": chunk[:30].strip().replace("\n", " ")}
            ))
    vectordb = FAISS.from_documents(chunks, embedding)
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
    ë‹¤ìŒì€ 'ìì·¨ë°±ê³¼ì‚¬ì „'ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.
    ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
    
    [ë¬¸ì„œ ë‚´ìš©]
    {context}
    
    [ì§ˆë¬¸]
    {question}
    
    [ë‹µë³€]
    """
    )
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0),
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    return qa


# âœ… í†µí•© ì±—ë´‡

def unified_chatbot(query: str) -> str:
    output = f""

    source = classify_query(query)
    if source == "csv":
        qa, id_to_raw = get_csv_qa("data/ì§ë°©ë°ì´í„°ì…‹.json", "./vectorstore", query)
        result = qa.invoke(query)

        for i, doc in enumerate(result['source_documents']):
            doc_id = doc.metadata.get("ë§¤ë¬¼ID")
            raw = id_to_raw.get(doc_id, {})
            output += f"â–¶ï¸ ë§¤ë¬¼ {i + 1}<br>"
            output += f"- ìœ„ì¹˜: {raw.get('ì£¼ì†Œ(ë²•ì •ë™)', 'ì •ë³´ ì—†ìŒ')} / ë³´ì¦ê¸ˆ: {raw.get('ë³´ì¦ê¸ˆ(ë§Œì›)', 'ì •ë³´ ì—†ìŒ')} / ì›”ì„¸: {raw.get('ì›”ì„¸(ë§Œì›)', 'ì •ë³´ ì—†ìŒ')}ë§Œì›<br>"
            output += f"- ì¶©ë¬´ë¡œì—­ê¹Œì§€ ì‹œê°„: {raw.get('ë§¤ë¬¼_ë¶€í„°_ì¶©ë¬´ë¡œ1ì¶œê¹Œì§€_ì‹œê°„_ë¶„', 'ì •ë³´ ì—†ìŒ')}ë¶„<br>"
            output += "-" * 80 + "<br>"
    else:
        qa = get_pdf_qa(r"data/ìì·¨ë°±ê³¼ì‚¬ì „.pdf")
        result = qa.invoke(query)
        output += result['result']

    return output
