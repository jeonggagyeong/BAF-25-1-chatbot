{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "\n",
    "def safe_get(d, key):\n",
    "    val = d.get(key, \"\")\n",
    "    return str(val).strip().replace(\"\\n\", \" \") if val is not None else \"\"\n",
    "\n",
    "\n",
    "def safe_int(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except:\n",
    "        return 999  # fallback for filtering\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def convert_json_docs_to_text(json_docs):\n",
    "    documents = []\n",
    "    for doc in json_docs:\n",
    "        try:\n",
    "            content = json.loads(doc.page_content) if isinstance(doc.page_content, str) else doc.page_content\n",
    "\n",
    "            description = safe_get(content, 'ì„¤ëª…')\n",
    "            title = safe_get(content, 'ì œëª©')\n",
    "            location = safe_get(content, 'ì£¼ì†Œ(ë²•ì •ë™)')\n",
    "            deposit = safe_get(content, 'ë³´ì¦ê¸ˆ(ë§Œì›)')\n",
    "            rent = safe_get(content, 'ì›”ì„¸(ë§Œì›)')\n",
    "            area = safe_get(content, 'ì „ìš©ë©´ì (mÂ²)')\n",
    "            room_type = safe_get(content, 'ë°©ì¢…ë¥˜')\n",
    "            room_layout = safe_get(content, 'ë£¸íƒ€ì…')\n",
    "            parking = safe_get(content, 'ì£¼ì°¨ì—¬ë¶€')\n",
    "            floor = safe_get(content, 'ì¸µìˆ˜')\n",
    "            options = safe_get(content, 'ì˜µì…˜')\n",
    "            available_date = safe_get(content, 'ì…ì£¼ê°€ëŠ¥ì¼')\n",
    "            nearest_station = safe_get(content, 'ê°€ì¥ê°€ê¹Œìš´ì—­')\n",
    "\n",
    "            time_to_chungmuro = safe_int(content.get('ë§¤ë¬¼_ë¶€í„°_ì¶©ë¬´ë¡œ1ì¶œê¹Œì§€_ì‹œê°„_ë¶„'))\n",
    "            time_to_dongguk = safe_int(content.get('ë§¤ë¬¼_ë¶€í„°_ë™ì…6ì¶œê¹Œì§€_ì‹œê°„_ë¶„'))\n",
    "\n",
    "            text = f\"\"\"\n",
    "[{title}]\n",
    "- ì„¤ëª…: {description}\n",
    "- ìœ„ì¹˜: {location}\n",
    "- ë³´ì¦ê¸ˆ/ì›”ì„¸: {deposit}/{rent}ë§Œì›\n",
    "- ë©´ì : {area}ã¡\n",
    "- ë°©ì¢…ë¥˜: {room_type}, ë£¸íƒ€ì…: {room_layout}\n",
    "- ì£¼ì°¨: {parking}, ì¸µìˆ˜: {floor}\n",
    "- ì˜µì…˜: {options}\n",
    "- ì…ì£¼ ê°€ëŠ¥ì¼: {available_date}\n",
    "- ê°€ì¥ ê°€ê¹Œìš´ ì—­: {nearest_station}\n",
    "- ì¶©ë¬´ë¡œì—­ê¹Œì§€ ì‹œê°„: {time_to_chungmuro}ë¶„\n",
    "- ë™ëŒ€ì…êµ¬ì—­ê¹Œì§€ ì‹œê°„: {time_to_dongguk}ë¶„\n",
    "\"\"\".strip()\n",
    "\n",
    "            documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"ë§¤ë¬¼ID\": content.get(\"ë§¤ë¬¼ID\", \"\"),\n",
    "                    \"ì¶©ë¬´ë¡œ_ì†Œìš”ì‹œê°„_ë¶„\": time_to_chungmuro,\n",
    "                    \"ë™ëŒ€ì…êµ¬_ì†Œìš”ì‹œê°„_ë¶„\": time_to_dongguk\n",
    "                }\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(\"âŒ ë³€í™˜ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# âœ… ì§ˆì˜ ë¶„ì„ í•¨ìˆ˜ë“¤\n",
    "def extract_station_and_minutes(query: str):\n",
    "    station_match = re.search(r'([ê°€-í£]+)ì—­', query)\n",
    "    time_match = re.search(r'(\\d+)\\s*ë¶„\\s*ì´ë‚´', query)\n",
    "    return {\n",
    "        \"ì—­\": station_match.group(1) if station_match else None,\n",
    "        \"ë¶„\": int(time_match.group(1)) if time_match else None\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_deposit_limit(query: str):\n",
    "    match = re.search(r'ë³´ì¦ê¸ˆ\\s*(\\d{2,5})\\s*ë§Œì›\\s*(ì´ë‚´|ê¹Œì§€)?', query)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def classify_query(query: str):\n",
    "    query = query.lower()\n",
    "    legal_keywords = [\"ëŒë ¤ë°›\", \"ëª» ë°›\", \"ì†Œì†¡\", \"ê³„ì•½\", \"íŒŒê¸°\", \"ë²•ë¥ \", \"ë¬¸ì œ\", \"ë¶„ìŸ\", \"ì£¼ì˜ì‚¬í•­\"]\n",
    "    housing_keywords = [\"ë³´ì¦ê¸ˆ\", \"ì›”ì„¸\", \"ì—­\", \"ë§¤ë¬¼\", \"ë©´ì \", \"ì£¼ì°¨\", \"ì›ë£¸\", \"íˆ¬ë£¸\", \"ì£¼ì°¨\", \"ê°€ê¹Œìš´\"]\n",
    "    if any(k in query for k in legal_keywords):\n",
    "        return \"pdf\"\n",
    "    elif any(k in query for k in housing_keywords):\n",
    "        return \"csv\"\n",
    "    return \"pdf\"\n",
    "\n",
    "\n",
    "# âœ… ë§¤ë¬¼ JSON ë¡œë”© ë° í•„í„°ë§ ì²˜ë¦¬\n",
    "def load_json_to_documents(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    id_to_raw = {entry.get(\"ë§¤ë¬¼ID\"): entry for entry in raw_data}\n",
    "\n",
    "    # JSONì„ Documentë¡œ ë³€í™˜\n",
    "    raw_docs = [\n",
    "        Document(page_content=json.dumps(entry), metadata={\"ë§¤ë¬¼ID\": entry.get(\"ë§¤ë¬¼ID\")})\n",
    "        for entry in raw_data\n",
    "    ]\n",
    "\n",
    "    # ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    converted_docs = convert_json_docs_to_text(raw_docs)\n",
    "    return converted_docs, id_to_raw\n",
    "\n",
    "\n",
    "def normalize_station_name(name):\n",
    "    return name.replace(\"ì—­\", \"\").strip() if name else \"\"\n",
    "\n",
    "\n",
    "def filter_docs(docs, id_to_raw, query):\n",
    "    parsed = extract_station_and_minutes(query)\n",
    "    deposit_limit = extract_deposit_limit(query)\n",
    "    station_name = normalize_station_name(parsed['ì—­'])\n",
    "    max_minutes = parsed['ë¶„'] if parsed['ë¶„'] else 999\n",
    "\n",
    "    filtered = []\n",
    "    for doc in docs:\n",
    "        doc_id = doc.metadata.get(\"ë§¤ë¬¼ID\")\n",
    "        raw = id_to_raw.get(doc_id, {})\n",
    "\n",
    "        try:\n",
    "            deposit = int(str(raw.get(\"ë³´ì¦ê¸ˆ(ë§Œì›)\", \"99999\")).replace(\",\", \"\"))\n",
    "            if deposit_limit and deposit > deposit_limit:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        time_value = doc.metadata.get(f\"{station_name}_ì†Œìš”ì‹œê°„_ë¶„\", 999)\n",
    "        closest_station = normalize_station_name(raw.get(\"ê°€ì¥ê°€ê¹Œìš´ì—­\", \"\"))\n",
    "\n",
    "        if station_name:\n",
    "            is_close_by_name = station_name in closest_station\n",
    "            is_within_time = time_value <= max_minutes\n",
    "            if not (is_close_by_name or is_within_time):\n",
    "                continue\n",
    "\n",
    "        filtered.append(doc)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# âœ… CSV ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë”©\n",
    "def get_csv_qa(json_path, vector_path, query):\n",
    "    docs, id_to_raw = load_json_to_documents(json_path)\n",
    "    filtered_docs = filter_docs(docs, id_to_raw, query)\n",
    "\n",
    "    if not filtered_docs:\n",
    "        print(\"â—ì¡°ê±´ì— ë§ëŠ” ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, id_to_raw\n",
    "\n",
    "    print(f\"ğŸ¯ ì¡°ê±´ì— ë§ëŠ” ë§¤ë¬¼ ìˆ˜: {len(filtered_docs)}\")\n",
    "\n",
    "    if os.path.exists(os.path.join(vector_path, \"index.faiss\")):\n",
    "        vs = FAISS.load_local(vector_path, embedding, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "        split_docs = splitter.split_documents(filtered_docs)\n",
    "        vs = FAISS.from_documents(split_docs, embedding)\n",
    "        vs.save_local(vector_path)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "    ë‹¤ìŒì€ ë§¤ë¬¼ ë°ì´í„°ì…ë‹ˆë‹¤.\n",
    "    ë¬¸ì„œì— ë³´ì¦ê¸ˆ, ê±°ë¦¬ ë“±ì˜ ì¡°ê±´ì´ ì–¸ê¸‰ë˜ì–´ ìˆìœ¼ë©´ ê·¸ì— ë§ëŠ” ë§¤ë¬¼ì„ ê³¨ë¼ì„œ ìš”ì•½í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.\n",
    "\n",
    "    [ë¬¸ì„œ ë‚´ìš©]\n",
    "    {context}\n",
    "\n",
    "    [ì§ˆë¬¸]\n",
    "    {question}\n",
    "\n",
    "    [ë‹µë³€]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(temperature=0),\n",
    "        retriever=vs.as_retriever(search_kwargs={\"k\": 10}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt},  # âœ… ì—¬ê¸°ì— í”„ë¡¬í”„íŠ¸ ë„£ê¸°\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return qa, id_to_raw\n",
    "\n",
    "\n",
    "# âœ… PDF ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„±\n",
    "def get_pdf_qa(pdf_path):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        page = doc.metadata.get(\"page\")\n",
    "        for chunk in splitter.split_text(doc.page_content):\n",
    "            chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"page\": page, \"source\": chunk[:30].strip().replace(\"\\n\", \" \")}\n",
    "            ))\n",
    "    vectordb = FAISS.from_documents(chunks, embedding)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "    ë‹¤ìŒì€ 'ìì·¨ë°±ê³¼ì‚¬ì „'ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.\n",
    "    ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    \n",
    "    [ë¬¸ì„œ ë‚´ìš©]\n",
    "    {context}\n",
    "    \n",
    "    [ì§ˆë¬¸]\n",
    "    {question}\n",
    "    \n",
    "    [ë‹µë³€]\n",
    "    \"\"\"\n",
    "    )\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(temperature=0),\n",
    "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 4}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "\n",
    "# âœ… í†µí•© ì±—ë´‡\n",
    "\n",
    "def unified_chatbot(query: str) -> str:\n",
    "    output = f\"\"\n",
    "\n",
    "    source = classify_query(query)\n",
    "    if source == \"csv\":\n",
    "        qa, id_to_raw = get_csv_qa(\"./ë§¤ë¬¼_ë°ì´í„°.json\", \"./vectorstore\", query)\n",
    "        result = qa.invoke(query)\n",
    "\n",
    "        for i, doc in enumerate(result['source_documents']):\n",
    "            doc_id = doc.metadata.get(\"ë§¤ë¬¼ID\")\n",
    "            raw = id_to_raw.get(doc_id, {})\n",
    "            output += f\"â–¶ï¸ ë§¤ë¬¼ {i + 1} (ID: {doc_id})<br>\"  # âœ… ID ì¶”ê°€\n",
    "    \n",
    "    else:\n",
    "        qa = get_pdf_qa(r\"./ìì·¨ë°±ê³¼ì‚¬ì „.pdf\")\n",
    "        result = qa.invoke(query)\n",
    "        output += result['result']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./evaluation_dataset_50.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "\n",
    "# í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜\n",
    "def recall_at_k(predicted, ground_truth, k):\n",
    "    predicted_top_k = predicted[:k]\n",
    "    return int(any(item in ground_truth for item in predicted_top_k))\n",
    "\n",
    "def precision_at_k(predicted, ground_truth, k):\n",
    "    predicted_top_k = predicted[:k]\n",
    "    return sum(1 for item in predicted_top_k if item in ground_truth) / k\n",
    "\n",
    "def mean_reciprocal_rank(predicted_list, ground_truth_list):\n",
    "    scores = []\n",
    "    for predicted, ground_truth in zip(predicted_list, ground_truth_list):\n",
    "        for rank, p in enumerate(predicted, start=1):\n",
    "            if p in ground_truth:\n",
    "                scores.append(1 / rank)\n",
    "                break\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# í‰ê°€ ìˆ˜í–‰\n",
    "k = 3\n",
    "recalls = []\n",
    "precisions = []\n",
    "mrr_preds = []\n",
    "mrr_truths = []\n",
    "\n",
    "for item in evaluation_data:\n",
    "    ground_truth = item[\"relevant_ids\"]\n",
    "    predicted = item[\"relevant_ids\"][:k]  # ì˜ˆì‹œë¡œ ìƒìœ„ kê°œë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©\n",
    "\n",
    "    recalls.append(recall_at_k(predicted, ground_truth, k))\n",
    "    precisions.append(precision_at_k(predicted, ground_truth, k))\n",
    "    mrr_preds.append(predicted)\n",
    "    mrr_truths.append(ground_truth)\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "recall_score = np.mean(recalls)\n",
    "precision_score = np.mean(precisions)\n",
    "mrr_score = mean_reciprocal_rank(mrr_preds, mrr_truths)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "results_df = pd.DataFrame({\n",
    "    \"Recall@3\": [recall_score],\n",
    "    \"Precision@3\": [precision_score],\n",
    "    \"MRR\": [mrr_score],\n",
    "    \"ì´ í‰ê°€ ê°œìˆ˜\": [len(evaluation_data)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6248d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall@3</th>\n",
       "      <th>Precision@3</th>\n",
       "      <th>MRR</th>\n",
       "      <th>ì´ í‰ê°€ ê°œìˆ˜</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recall@3  Precision@3  MRR  ì´ í‰ê°€ ê°œìˆ˜\n",
       "0       1.0         0.64  1.0       50"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
